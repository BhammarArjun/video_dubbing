# ðŸ§© **How the Multilingual Dubbing Pipeline Thinks**

video demo - https://drive.google.com/file/d/1rn-Uv56FF3FwM8yEWOEv5-uahPEU3qJF/view?usp=sharing

I wanted a system that turns *any* YouTube video into a naturalâ€‘sounding dub in another language while preserving each speakerâ€™s vibe. Below is the mental wiring diagramâ€”why each choice was made and what still hurts.

---

## 1â€‚Gemini as the Brains of ASR

### What Gemini actually does

1. **Watches the raw YouTube URL** â€“ Because Gemini accepts vision + audio, I skip downloading giant audio tracks.
2. **Diarises up to four main speakers (+â€¯â€œOther Speakersâ€)** â€“ Assigns IDs and *perceives* gender.
3. **Transcribes in the source language** â€“ Chunked by *meaning*, not arbitrary timestamps.
4. **Translates each chunk into the target language** â€“ While mirroring each speakerâ€™s formality, slang, or humor.
5. **Builds a ********************************`tts_instruction_key`******************************** for every speaker** â€“ Voice quality, tone, pacing, accent quirks, etc.
6. **Returns one strict JSON** â€“ Keys: `speaker_profiles` and `dubbing_segments`; timestamps in MM\:SS\:mmm; durations to 3â€¯dp.

### Why the prompt looks the way it does

* **Speaker cap** â†’ 4 IDs keeps voice management feasible.
* **Gender tag** â†’ Lets me map to male/female voice buckets quickly.
* **Style keys** â†’ Without them, TTS voices sound generic.
* **Chunking rule** â†’ â€œDonâ€™t split continuous thoughtsâ€ prevents robotic midâ€‘sentence cuts.
* **Absolute timestamp requirement** â†’ Crucial for later lipâ€‘sync.

*(Full prompt lives in ****************`config/prompts.py`****************; variables stripped here for clarity.)*

---

## 2â€‚Turning Text into Voice

1. **Voice pick** â€“ For each chunk I look at its speakerâ€™s gender tag and pick a GPTâ€‘4o TTS voice from a predefined map. (Female â†’ `nova`, `shimmer`; Male â†’ `alloy`, `echo`.)
2. **Inject style** â€“ The chunk text plus its `tts_instruction_key` go straight into the TTS call, so tone and pacing match the original.
3. **Speed mismatch problem** â€“ Target language sentences almost never last the exact same time. After synthesis I measure

   ```
   ratio = len(audio) / (duration_seconds * sample_rate)
   ```

   and WSOLAâ€‘stretch between **0.5â€¯Ã— and 2â€¯Ã—** so the start/end align.
4. **Quality hit** â€“ WSOLA is gentler than naÃ¯ve resampling but artifacts creep in beyond Â±25â€¯%. Real fix =

   * *Voiceâ€‘toâ€‘voice* models (clone the original voice, respect timing).
   * or **TTS with duration control** (tell the model: â€œSpeak this in 2.37â€¯sâ€).

### Why not ElevenLabs (yet)?

* It offers great cloning, but I found no reliable way to pass a full *style input + precise duration hints in one shot.*
* GPTâ€‘4o TTS isnâ€™t perfect, but its  input lets me stuff the entire `tts_instruction_key` directly.

### Why not Gemini TTS?

* It *can* read the same JSON and nail tone, but freeâ€‘tier rate limits stall long videos. Also still no hard duration control.

---

## 3â€‚Merging Audio & Dealing with Background Music

* After speedâ€‘fixing, chunk WAVs are concatenated â†’ one dub track.
* FFmpeg replaces the original audio in the MP4.
* **Missing BGM issue:**  YouTube  streams come without the music bed. Thatâ€™s okay for tutorials; bad for movies. Future plan: download separate audioâ€‘only stream or run sourceâ€‘separation, then reâ€‘mix music under the dub.

---

## 4â€‚Current Pain Points & Wishlist

| Pain                   | Dream Fix                                                   |
| ---------------------- | ----------------------------------------------------------- |
| Timeâ€‘stretch artifacts | Voiceâ€‘toâ€‘voice with builtâ€‘in alignment                      |
| Lost background music  | Adaptive audio download + ducking under synthesized speech. |
| API rate limits        | Batch jobs, paid tiers, or local models.                    |

---

## 5â€‚Roadmap of Experiments

1. Will Try outÂ VoiceStar and Gemini TTS in future

---

## 6â€‚Credit

* **Arjun Bhammar** â€“ Original concept & code.
* **ChatGPT** â€“ Scribe of these thoughts.
* **Google DeepMind & OpenAI** â€“ The engines.

> *README autoâ€‘generated by ChatGPT because typing Markdown bores me more than debugging FFmpeg.*
> Check run.md file to setup instructions
